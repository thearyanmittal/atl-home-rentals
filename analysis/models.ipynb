{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataset\n",
    "df = pd.read_pickle('../data/data.pkl')\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function (root mean squared error)\n",
    "def rmse(model):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    return np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of models to evaluate (GridSearchCV was used to find the best parameters for each model type)\n",
    "models = [\n",
    "    Lasso(alpha=2.173),\n",
    "    Ridge(alpha=24.995),\n",
    "    ElasticNet(alpha=.733, l1_ratio=.992),\n",
    "    SVR(kernel='linear'),\n",
    "    RandomForestRegressor(n_estimators=1800, max_features=.333, max_depth=None),\n",
    "    AdaBoostRegressor(n_estimators=100, learning_rate=.05),\n",
    "    GradientBoostingRegressor(n_estimators=600, max_depth=4, learning_rate=.05, max_features='sqrt', loss='squared_error'),\n",
    "    XGBRegressor(colsample_bytree=.1, learning_rate=.05, max_depth=4, n_estimators=1000, reg_alpha=.5, reg_lambda=.25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using GridSearchCV to tune hyperparameters\n",
    "\n",
    "# params = {\n",
    "#     'max_depth': (3, 4, None),\n",
    "#     'n_estimators': (500, 1000, 2000),\n",
    "#     'colsample_bytree': (.1, .333, .5),\n",
    "#     'reg_alpha': (.25, .5, 75),\n",
    "#     'reg_lambda': (.25, .5, .75)\n",
    "# }\n",
    "# reg = GridSearchCV(XGBRegressor(learning_rate=.05), params, n_jobs=-1)\n",
    "# reg.fit(X, y)\n",
    "# reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMS Error: 677.734774857363 ± 33.427346359761515\n",
      "Ridge RMS Error: 676.4928888133803 ± 36.93320032272733\n",
      "ElasticNet RMS Error: 675.3574861650643 ± 36.11773111759448\n",
      "SVR RMS Error: 835.2549960323838 ± 38.13046119892962\n",
      "RandomForestRegressor RMS Error: 621.3633780947794 ± 35.48021162876483\n",
      "AdaBoostRegressor RMS Error: 771.1188945457418 ± 29.903099422023182\n",
      "GradientBoostingRegressor RMS Error: 590.6028157085999 ± 31.311346276600183\n",
      "XGBRegressor RMS Error: 576.4541533015773 ± 35.790300554675454\n"
     ]
    }
   ],
   "source": [
    "# create and evaluate models\n",
    "for model in models:\n",
    "    score = rmse(model)\n",
    "    print(f\"{str(model).split('(')[0]} RMS Error: {score.mean()} \\u00B1 {score.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3617.000000\n",
       "mean      2245.439591\n",
       "std        998.177401\n",
       "min        600.000000\n",
       "25%       1695.000000\n",
       "50%       1995.000000\n",
       "75%       2495.000000\n",
       "max      10000.000000\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare the regression results with the price range of the datapoints\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors and Coefficients: \n",
      "Full_Baths       1190.423131\n",
      "SQFT_Int         1145.226357\n",
      "Median Income     722.216302\n",
      "Age               539.660418\n",
      "Grill             463.367039\n",
      "dtype: float64\n",
      "\n",
      "Bottom Factors and Coefficients: \n",
      "Style: Brick Front            -99.867705\n",
      "Lot_Size_Num                 -102.058313\n",
      "Amenities: Tennis Court(s)   -107.112452\n",
      "Households                   -156.347660\n",
      "Rooms: Game                  -271.265033\n",
      "dtype: float64\n",
      "\n",
      "74 Zero-Coefficient Factors (out of 177)\n"
     ]
    }
   ],
   "source": [
    "# analyze coefficients of regression to see which factors are weighted most heavily\n",
    "\n",
    "lasso = Lasso(alpha=2.173).fit(X, y)\n",
    "coef = pd.Series(lasso.coef_, index=X.columns).sort_values(ascending=False)\n",
    "print(f'Top Factors and Coefficients: \\n{coef[:5]}')\n",
    "print(f'\\nBottom Factors and Coefficients: \\n{coef[-5:]}')\n",
    "print(f'\\n{len(coef[coef == 0])} Zero-Coefficient Factors (out of 177)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "The XGBoost Regressor by far performed the best on this dataset. However, even its root mean square error was about 576 USD. While this model can give a reasonable estimate regarding the price range of a certain rental, it by no means should be used as an exact price calculator. The standard deviation in the error across all folds is only about 32 USD, though, indicating that the model generalizes well.\n",
    "\n",
    "As expected, the Lasso model highly weights factors that were identified in the exploratory phase to have the greatest positive correlation with price, such as the number of full baths, the square footage of the home, and the median income of the area. Interestingly, the model assigns a large negative coefficient to the size of the lot, perhaps indicating that home rentals on large estates go for lower prices. The model also zeroes out 74/177 variables, indicating that only about 100 or fewer of the columns were absolutely necessary to accurately predict pricing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('atl-home-rentals-sIrw7Atm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ae51bed731bf37f97494d6502d8927d782a1982e3461efeaf84bb1330b038a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
